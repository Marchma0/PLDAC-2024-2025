# Compte Rendu post-réunion de la semaine (14/02)


## SARSA
L'algo de SARSA utilise un algorithme (epsilon-greedy ou softmax par exemple) ou suit la policy que l'on connaît déjà pour choisir l'action t et l'action t+1. Attention à ne pas le recalculer au prochain pas de temps.

## Algos de choix d'actions
- **Epsilon-greedy** : La probabilité de choisir l'action optimale est epsilon et la probabilité de choisir les autres actions est \(1 - \frac{\epsilon}{\text{nb\_actions}}\). Très apprécié par les mathématiciens, mais rend difficile l'exploration.
- **Roulette wheel** : Calculer la probabilité/la somme des probabilités puis tirer. Plus équitable et permet l'exploration, mais n'avantage pas assez l'optimalité.
- **Softmax** (utiliser une fonction pour récupérer une action car ne donne que les probas) : Donne les probabilités en faisant l'exponentielle de la Q-value puis en divisant par la somme des exponentielles. Avec un paramètre beta, on peut choisir la "température" de la distribution. Plus beta est grand, plus la distribution est uniforme, facilitant l'exploration (si beta tend vers l'infini, on a des probas \( \frac{1}{\text{nb\_probs}} \)). Plus beta est petit, plus la distribution est proche de l'optimalité, et on réduit l'exploration. On a donc un contrôle sur l'exploration et l'exploitation, contrairement aux 2 autres algos.

## BBRL
**BBRL** (Blackboard Reinforcement Learning) est un algorithme qui permet de raccourcir le temps d'apprentissage d'un agent. Il permet de gérer les données et de les organiser pour que l'apprentissage soit parallélisé, permettant ainsi à l'agent d'apprendre sur plusieurs environnements en parallèle.

BBRL utilise un **workspace** : `dict()` qui contient des données pour chaque environnement. Il stocke les observations, les actions, les rewards et les done pour chaque environnement.

Il y a 2 types d'agents :
- Un agent **action** qui effectue des actions à partir des observations.
- Un agent **environnement** qui observe les actions des agents actions (?) et envoie les rewards.

BBRL fonctionne avec des tenseurs permettant d'utiliser les fonctionnalités de PyTorch pour augmenter la vitesse de calcul. Les agents de BBRL sont des `nn.Module` de PyTorch et permettent le calcul avec GPU.

Dans BBRL, il y a une fonctionnalité de **auto-reset** qui permet de choisir si l'on doit attendre la fin de tous les environnements pour recommencer ou si l'on doit recommencer dès qu'un environnement est fini.  
On utilise `auto-reset = True` pour l'entraînement et `auto-reset = False` pour tester les policies apprises.

## Pour la semaine prochaine
- Maîtriser BBRL en finissant les notebooks (1,2 et 3).
- Regarder la vidéo sur DQN et comprendre son fonctionnement.
