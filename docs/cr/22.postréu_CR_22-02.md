# Compte Rendu post-réunion de la semaine (22/02)

## Environnements Parallèles et Auto-reset
  - L'agent interagit simultanément avec plusieurs environnements.
  - On utilise des approches multiprocessing ou single process pour exploiter les opérations vectorielles (notamment avec GPU).
  - Permet d'accélérer la collecte et d'améliorer la stabilité de l'apprentissage.

## Problèmes liés à la synchronisation des épisodes
  - Certains épisodes durent très peu de pas alors que d'autres peuvent durer jusqu'à 500 pas :

### Modes Auto-reset
- **Autoreset=False :**
  - Un environnement arrêté attend que tous les autres terminent leur épisode.
  - Simple mais beaucoup de pertes de temps donc pas très optimal.
- **Autoreset=True :**
  - Dès qu'un environnement termine un épisode, il se réinitialise immédiatement (plus de temps mort)
  - Par contre, il faut gérer les transitions incomplètes en fin d'époque pour ne pas perdre de données pertinentes.

## Gestion des Transitions et Liaison entre Époques
  - Copier la dernière étape d'une époque dans la première étape de l'époque suivante (via `train_workspace.copy_n_last_steps(1)`) afin de ne pas perdre de transition.
  - Les transitions qui passent de la fin d'un épisode au début d'un nouveau ne doivent pas être utilisées pour l'entraînement.
  - Le processus de duplication en paires dans `workspace.get_transitions()` permet d'identifier et de supprimer ces transitions indésirables.
  
## Wrapper
  - Fonctionne comme un design pattern decorator : nous permets donc de modifier le comportement d'environnement.

## DQN

  - Utilise un réseau de neurones pour approximer la fonction de valeur Q, qui prédit les récompenses futures d’une action donnée dans un état spécifique d’un environnement. 
  - Pour cela on utilise presque comme l'apprentissage supervisé : on minimise une fonction de cout (sauf que dans ce cas, yhat utilise notre prédiciton)
  - Pour stabiliser l'apprentissage, DQN utilise des techniques : 
    1. stable target q-function : fixer yhat pendant plusieurs itérations pour pouvoir converger.
    2. replay buffer shuffling : mélanger nos observations pour avoir iid et créer nos mini batch plus efficacement.

## Pour la semaine prochaine
- Finir les notebooks sur DQN
- Mettre en place un environnement de travail sur github