# Compte Rendu pré-réunion de la semaine (22/02)

## Objectifs complétés
1. **Lecture des cours sur la bibliothèque BBRL**
   - `Auto-reset = False`
   - `Auto-reset = True`
   - Time limits, breaks

2. **Complétion des notebooks sur la bibliothèque BBRL**
   - Introduction : classe **Agent** et lancement d'environnements avec **Workspace** (PowerAgent et CollatzAgent)
   - `autoreset = False` : Test sur le **RandomAgent** et création d'un agent qui ne fait que l'action 1 (**StupidAgent**).
   - `autoreset = True` : Test sur le **RandomAgent** et **ReplayBuffer**.

3. **Lecture des slides et visionnage de la vidéo sur DQN**
   - **DQN** : Deep Q-Network
   - Rapprochement avec **Q-Learning**
   - Apprentissage de la fonction **Q** par réseau de neurones
   - Tricks pour l'apprentissage :
     - Stable Target
     - Replay Buffer Shuffling
     - Double Q-Learning
     - Prioritized Replay Buffer
     - Dueling networks (Advantage function)
   - (Rapide lecture des slides sur **Rainbow**)
   - (Lecture et début du notebook sur **DQN**)

---

## Questions
1. A quoi sert la propagation des valeurs du prochain state dans le state actuel quand le time limit est atteint ?
2. Pouvez-vous nous expliquer comment fonctionne un **Q-network** exactement (nœuds d'entrée et nœuds cachés) ?
3. Pouvez-vous nous expliquer en quoi l'utilisation du **Replay Buffer shuffling** permet-elle de rendre l'apprentissage plus stable ?
4. Peut-on avoir une courte explication sur le **Double Q-Learning** ?
5. Comment est-ce que le **Dueling Network** arrive à mieux séparer les aspects de l'état et de l'action dans l'apprentissage de la fonction Q ?
6. Pouvez-vous nous faire un court résumé du passage de **DQN** à **Rainbow** et en quoi cela améliore l'apprentissage ? (Y a-t-il meilleur que Rainbow aujourd'hui ?)
