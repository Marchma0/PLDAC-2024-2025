# Compte Rendu pré-réunion de la semaine (14/02)

## Objectifs Complétés
1. **Tabular Dynamic Programming**
   - MDP (Markov Decision Process)
   - Agrégation criterion
   - Value function & action value function
   - Value iteration algorithm
   - Policy iteration
   - Epsilon-greedy strategy

2. **Tabular Model-Free Reinforcement Learning**
   - Incremental estimation: \( E_{k+1}(s) = E_k(s) + \alpha[r_{k+1} - E_k(s)] \)
   - Temporal Difference Error: \( \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \)
   - Policy evaluation ?
   - Sarsa ?
   - Q-Learning
   - Actor-Critic

## Bibliothèque BBRL
- Prise en main de la bibliothèque
- Début du notebook (Fibonacci et Power2)
- Création de l’Environnement de Travail
- Clonage du dépôt Git
- Quelques tests effectués

---

## Questions
- Qu’est-ce que la **Policy Evaluation** ?
- Implémentation et compréhension de **Sarsa** (différence avec Q-learning) ?
- Pouvez-vous nous faire un petit résumé des fonctionnalités de **BBRL** (on a vraiment du mal avec, ex. terminationChecker) ?
- Pourquoi utilise-t-on des arrays dans des tensors ?
- Après avoir effectué des tests avec l’algorithme **AFU**, nous avons remarqué que nous n’avions pas de représentation graphique. Est-ce que cela nous aiderait d’en avoir une pour mieux comprendre ?
- Environnement de travail : GitHub, notebook, etc.
